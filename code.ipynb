{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_6Fsl7NQw0K"
      },
      "source": [
        "### **Reading from the file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bil9bbngPjHd"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from statistics import harmonic_mean as hm\n",
        "import math\n",
        "import pandas as pd\n",
        "from math import log10\n",
        "\n",
        "def get_average(l):\n",
        "  return sum(l)/len(l)\n",
        "\n",
        "file_path = \"/content/drive/My Drive/Temp/brown.txt\" \n",
        "file = open(file_path, \"r\")\n",
        "if file:\n",
        "\ttext = file.read()\n",
        "\n",
        "lines = text.split(\"\\n\")\n",
        "print(lines[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFt5xHJfQuXg"
      },
      "source": [
        "### **Processing the lines**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFlvr43_QU5x"
      },
      "source": [
        "processed_lines = []\n",
        "for i in lines:\n",
        "  processed_tokens=[]\n",
        "  tokens = i.split(\" \")\n",
        "  if len(tokens)>1:\n",
        "    for j in tokens:\n",
        "      processed_word_tag=\"######\"\n",
        "      word_tag = j.split(\"_\")\n",
        "      if len(word_tag)==2:\n",
        "        tag = re.split(\"-HL|-TL|-NC\",word_tag[1])[0]\n",
        "        tag = tag.split(\"FW-\")[-1]\n",
        "        processed_word_tag = word_tag[0].lower()+\"_\"+tag\n",
        "        processed_tokens.append(processed_word_tag)\n",
        "    start = [\"^_^\"]\n",
        "    end = [\"._.\"]\n",
        "    sent = start + processed_tokens\n",
        "    if processed_tokens[-1]!=end[0]:\n",
        "      sent+=end\n",
        "    processed_lines.append(sent)\n",
        "  \n",
        "  \n",
        "print(processed_lines[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lTQQqgATKMI"
      },
      "source": [
        "### **Partitioning the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojtrnnTnQtgb"
      },
      "source": [
        "print(len(processed_lines))\n",
        "length = len(processed_lines)\n",
        "mod = length%3\n",
        "part = int((length-mod)/3)\n",
        "part1 = processed_lines[:part+mod]\n",
        "part2 = processed_lines[part+mod:(2*part)+mod]\n",
        "part3 = processed_lines[(2*part)+mod:]\n",
        "parts = [part1,part2,part3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWPVnfS-TY0H"
      },
      "source": [
        "# **Markov Assumption length 1** \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnlG3tPSTfCY"
      },
      "source": [
        "matrices_across_folds = {}\n",
        "overall_matrices_across_folds = {}\n",
        "frequnecy_distribution_across_folds = {}\n",
        "percentage_distribution_across_folds = {}\n",
        "ambiguous_matrix_across_folds = {}\n",
        "\n",
        "for phase in range(3): # 0,1,2\n",
        "\ttrain = parts[(phase)%3] + parts[(phase+1)%3]\n",
        "\ttest = parts[(phase+2)%3]\n",
        "\n",
        "\t#===========================================================\n",
        "\t#\t\t\t\t\tTRAINING\n",
        "\t#===========================================================\n",
        "\n",
        "\t# PRE-PROCESSING OF TOKENS\n",
        "\ttag_set = set([])  # all the tags from train data\n",
        "\tword_set = set([]) # all words from train data\n",
        "\tfor i in train:\n",
        "\t\ttokens = i\n",
        "\t\tfor j in tokens:\n",
        "\t\t\tword_tag = j.split(\"_\")\n",
        "\t\t\ttag_set.add(word_tag[1])\n",
        "\t\t\tword_set.add(word_tag[0]) \n",
        "\n",
        "\ttags = list(tag_set)\n",
        "\ttemplate_tags = []\n",
        "\ttags_index = {}\n",
        "\tl = len(tags)\n",
        "\tfor i in range(l):\n",
        "\t\ttemplate_tags.append(0)\n",
        "\t\ttags_index[tags[i]] = i\n",
        "\n",
        "\twords = list(word_set)\n",
        "\ttemplate_words = []\n",
        "\twords_index = {}\n",
        "\tl = len(words)\n",
        "\tfor i in range(l):\n",
        "\t\ttemplate_words.append(0)\n",
        "\t\twords_index[words[i]] = i\n",
        "\n",
        "\t# FINDING EMISSION AND TRANSITION COUNTS\n",
        "\n",
        "\temission_counts = {} # tag : [countS corresponding to all words]\n",
        "\t# total_counts_for_all_tags = template_tags[:] #[count of occurence of all tags]\n",
        "\ttotal_counts_for_all_tags = {} \n",
        "\n",
        "\tfor i in tags:\n",
        "\t  emission_counts[i] = template_words[:]\n",
        "\n",
        "\ttotal_trans = 0 \n",
        "\ttransition_counts = {} # {state1:  {state2 : count }}\n",
        "\n",
        "\tambiguity={}\n",
        "\toccurences={}\n",
        "\n",
        "\tfor i in tags:\n",
        "\t  temp = {}\n",
        "\t  for j in tags:\n",
        "\t    temp[j]=0\n",
        "\t  transition_counts[i]=temp\n",
        "\n",
        "\tfor i in train:\n",
        "\t  tokens = i\n",
        "\t  for j in range(len(tokens)-1):\n",
        "\t    token = tokens[j]\n",
        "\t    word_tag = token.split(\"_\")\n",
        "\t    s1 = word_tag[1]\n",
        "\t    token = tokens[j+1]\n",
        "\t    word_tag = token.split(\"_\")\n",
        "\t    s2 = word_tag[1]\n",
        "\t    total_trans += 1\n",
        "\t    transition_counts[s1][s2] += 1\n",
        "\n",
        "\tfor i in train:\n",
        "\t  tokens = i\n",
        "\t  for j in tokens:\n",
        "\t    word_tag = j.split(\"_\")\n",
        "\t    word = word_tag[0]\n",
        "\t    tag = word_tag[1]\n",
        "\t    index = words_index[word]\n",
        "\t    emission_counts[tag][index] += 1\n",
        "\t    index = tags_index[tag]\n",
        "\t    total_counts_for_all_tags[tag] = total_counts_for_all_tags.get(tag,0)+1\n",
        "\t    if word in occurences:\n",
        "\t      occurences[word]+=1\n",
        "\t      ambiguity[word].add(tag)\n",
        "\t    else:\n",
        "\t      occurences[word]=1\n",
        "\t      ambiguity[word]=set([tag])\n",
        "\n",
        "\t#===========================================================\n",
        "\t#\t\t\t\t\tTESTING DATA IS SEEN HERE\n",
        "\t#===========================================================\n",
        "\tnew_words = set([]) # have zero occurence counts\n",
        "\tnew_tags = set([]) # have zero occurence counts\n",
        "\n",
        "\t# SEPARATING WORDS AND TAGS OF THE TEST DATA\n",
        "\ttest_data=[]\n",
        "\tactual_tags = []\n",
        "\tfor i in test:\n",
        "\t  tokens = i\n",
        "\t  words=[]\n",
        "\t  tags=[]\n",
        "\t  for j in tokens:\n",
        "\t    word_tag = j.split(\"_\")\n",
        "\t    word = word_tag[0]\n",
        "\t    tag = word_tag[1]\n",
        "\t    words.append(word)\n",
        "\t    tags.append(tag)\n",
        "\t    if word not in word_set:\n",
        "\t      new_words.add(word)\n",
        "\t    if tag not in tag_set:\n",
        "\t      new_tags.add(tag)\n",
        "\t  test_data.append(words)\n",
        "\t  actual_tags.append(tags)\n",
        "\n",
        "\t# FINDING TOTAL SIZES OF VOCABULARIES\n",
        "\tall_words = set(list(word_set)+list(new_words))\n",
        "\tvocab_of_words = len(all_words)\n",
        "\tall_tags = set(list(tag_set)+list(new_tags))\n",
        "\tvocab_of_tags = len(all_tags)\n",
        "\t# transition_size = (len(all_tags))**2\n",
        "\n",
        "\t#===========================================================\n",
        "\t#\t\tCALCULATING PROBABILITIES WITH SMOOTHING\n",
        "\t#===========================================================\n",
        "\ttotal_tag_counts = sum(list(total_counts_for_all_tags.values()))\n",
        "\n",
        "\t# PROBABILITY OF OCCURENCE OF A TAG\n",
        "\ttag_probabs = {} \n",
        "\tfor k in emission_counts.keys(): # k = tag\n",
        "\t  v = sum(emission_counts[k])\n",
        "\t  tag_probabs[k] = (v+1)/(total_tag_counts + vocab_of_tags)\n",
        "\tfor i in new_tags:\n",
        "\t  tag_probabs[i] = 1/(total_tag_counts + vocab_of_tags)\n",
        "\n",
        "\t# PROBABILITY OF A WORD GIVEN A TAG IN LOG SPACE\n",
        "\tword_given_tag_probabs={}  #tag: {word : count}\n",
        "\tfor i in tag_set:\n",
        "\t  word_probabs={}\n",
        "\t  for j in word_set:\n",
        "\t    index = words_index[j]\n",
        "\t    word_probabs[j] = log10((emission_counts[i][index] +1)/(tag_probabs[i]*total_tag_counts + vocab_of_words))\n",
        "\t  for j in new_words:\n",
        "\t    word_probabs[j] = log10(1/(tag_probabs[i]*total_tag_counts + vocab_of_words))\n",
        "\t  word_given_tag_probabs[i] = word_probabs\n",
        "\n",
        "\tfor i in new_tags:\n",
        "\t  word_probabs={}\n",
        "\t  for j in all_words:\n",
        "\t    word_probabs[j] = log10(1/(tag_probabs[i]*total_tag_counts + vocab_of_words))\n",
        "\t  word_given_tag_probabs[i] = word_probabs\n",
        "\n",
        "\n",
        "\t# PROBABILITY OF A STATE(SAY 2) GIVEN PREVIOUS STATE(SAY 1) IN LOG SPACE\n",
        "\ts2_given_s1_probabs={} #tag2: {tag1 : count}\n",
        "\tfor t2 in tag_set:\n",
        "\t  t1_probabs={}\n",
        "\t  for t1 in tag_set:\n",
        "\t    t1_probabs[t1] = log10((transition_counts[t1][t2] + 1)/(tag_probabs[t1]*total_tag_counts + total_trans))\n",
        "\t  for t1 in new_tags:\n",
        "\t    t1_probabs[t1] = log10(1/(tag_probabs[t1]*total_tag_counts + total_trans))\n",
        "\t  s2_given_s1_probabs[t2] = t1_probabs\n",
        "\n",
        "\tfor t2 in new_tags:\n",
        "\t  t1_probabs={}\n",
        "\t  for t1 in all_tags:\n",
        "\t    t1_probabs[t1] = log10(1/(tag_probabs[t1]*total_tag_counts + total_trans))\n",
        "\t  s2_given_s1_probabs[t2] = t1_probabs\n",
        "\n",
        "\t#===========================================================\n",
        "\t#\t\t\tCALCULATING STATS OF TAGS IN VOCAB\n",
        "\t#===========================================================\n",
        "\n",
        "\tfor tag in new_tags:\n",
        "\t  total_counts_for_all_tags[tag] = 0 #frequencies\n",
        "\n",
        "\tprint(\"Statistics of tags: frequency distribution:\")\n",
        "\tprint(total_counts_for_all_tags)\n",
        "\tfrequnecy_distribution_across_folds[phase] = total_counts_for_all_tags\n",
        "\n",
        "\tpercentages={}\n",
        "\tfor i in all_tags:\n",
        "\t  percentages[i] = (total_counts_for_all_tags[i] * 100)/total_tag_counts\n",
        "\tprint(\"Statistics of tags: percentage distribution:\")\n",
        "\tprint(percentages)\n",
        "\tpercentage_distribution_across_folds[phase] = percentages\n",
        "\n",
        "\n",
        "\tambiguous={}\n",
        "\tfor k in ambiguity.keys():\n",
        "\t  if len(ambiguity[k])>1:\n",
        "\t    ambiguous[k]=len(ambiguity[k])\n",
        "\n",
        "\ttotal_occurences = sum(list(occurences.values()))\n",
        "\tambiguous_occurences={}\n",
        "\tfor k in ambiguous.keys():\n",
        "\t  ambiguous_occurences[(k,ambiguous[k])] = (100*occurences[k])/total_occurences\n",
        "\t# print(\"Words | Tags | Percentage of occurences\")\n",
        "\t# print(ambiguous_occurences)\n",
        "\ta=(len(ambiguous)*100)/len(all_words)\n",
        "\tb=sum(list(ambiguous_occurences.values()))\n",
        "\tprint(\"Percentage of words that are ambiguous\")\n",
        "\tprint(a)\n",
        "\tprint(\"Total occurences percentage of ambiguous words\")\n",
        "\tprint(b)\n",
        "\tambiguous_matrix_across_folds[phase] = [a,b]\n",
        "\n",
        "\t#===========================================================\n",
        "\t#\t\t\t\t\tTESTING BEGINS\n",
        "\t#===========================================================\n",
        "\tresults=[]\n",
        "\tfor i in test_data:\n",
        "\t  result=[]\n",
        "\t  tokens = i\n",
        "\t  entry_tag = tokens[0] # THIS IS ALWAYS ^\n",
        "\t  probabilities = {} \n",
        "\t  for t in all_tags:\n",
        "\t    probabilities[t] = s2_given_s1_probabs[t][entry_tag] # PROBABILITY OF TAG(i) GIVEN PREVIOUS TAG WAS ^\n",
        "\n",
        "\t  # TO FIND P(s|o)\n",
        "\t  for j in range(len(tokens)):\n",
        "\t    old_word = tokens[j]\n",
        "\t    # GIVES P(o|s)\n",
        "\t    probab_word_given_tags={}\n",
        "\t    for t in all_tags:\n",
        "\t      probab_word_given_tags[t] = word_given_tag_probabs[t][old_word]\n",
        "\n",
        "\t    # P(s2|s1) ARE ALREADY CALCULATED AND STORED IN s2_given_s1_probabs\n",
        "\t    P = {} # {s1 : p(o1|s1)*p(older(till s1))}\n",
        "\t    for k in probabilities.keys(): \n",
        "\t      tags = k.split(\" \")\n",
        "\t      last_tag = tags[-1] # gives s1\n",
        "\t      P[last_tag] = probabilities[k] + probab_word_given_tags[last_tag]\n",
        "\t      \n",
        "\t    # COMPUTING BEST PATH REACHING EACH TAG    \n",
        "\t    next_probabilities={} # {best path to tag : probability}\n",
        "\t    for t2 in all_tags:\n",
        "\t      probabs = {} # probabilites of all paths reaching t2\n",
        "\t      for k in probabilities.keys():\n",
        "\t        v = probabilities[k]\n",
        "\t        l = k.split(\" \")\n",
        "\t        t1 = l[-1]\n",
        "\t        probabs[k] = P[t1] + s2_given_s1_probabs[t2][t1]\n",
        "\t    \n",
        "\t      key_max = max(probabs, key=probabs.get) # max probability, best path\n",
        "\t      value_max = probabs[key_max]\n",
        "\t      l = key_max.split(\" \")\n",
        "\t      l.append(t2) # t2 is added to choosen path\n",
        "\t      v = value_max \n",
        "\t      k = \" \".join(i for i in l)\n",
        "\t      next_probabilities[k]=v\n",
        "\t    probabilities = next_probabilities\n",
        "\t    \n",
        "\t  key_max = max(probabilities, key=probabilities.get) # best tag set\n",
        "\t  result = key_max\n",
        "\t  results.append(result.split(\" \")[:-1])\n",
        "\t# for i in range(5):\n",
        "\t#   print(test_data[i])\n",
        "\t#   print(actual_tags[i])\n",
        "\t#   print(results[i])\n",
        "\n",
        "\n",
        "\t#===========================================================\n",
        "\t#\t\t\t\tCREATING CONFUSION MATRIX\n",
        "\t#===========================================================\n",
        "\n",
        "\tall_tags_index = {}\n",
        "\tall_tags_list = list(all_tags)\n",
        "\tall_tags_list.sort()\n",
        "\n",
        "\tfor i in range(len(all_tags)):\n",
        "\t  all_tags_index[all_tags_list[i]] = i\n",
        "\n",
        "\tdimension = len(all_tags)\n",
        "\tconfusion_matrix = np.zeros((dimension,dimension))\n",
        "\n",
        "\tfor i in range(len(actual_tags)):\n",
        "\t  for j in range(len(actual_tags[i])):\n",
        "\t    at = actual_tags[i][j]\n",
        "\t    pt = results[i][j]\n",
        "\t    ati = all_tags_index[at]\n",
        "\t    pti = all_tags_index[pt]\n",
        "\t    confusion_matrix[ati][pti]+=1\n",
        "\n",
        "\ttotal = np.sum(confusion_matrix)\n",
        "\tpredicted_totals = np.sum(confusion_matrix, axis = 0)\n",
        "\tactual_totals = np.sum(confusion_matrix, axis = 1)\n",
        "\n",
        "\t#===========================================================\n",
        "\t#\t\t\t\tFINDING TP, FP,FN FOR ALL TAGS\n",
        "\t#===========================================================\n",
        "\n",
        "\t# TP = matrix(tag,tag)\n",
        "\t# FP = predicted - TP\n",
        "\t# FN = actual - TP\n",
        "\n",
        "\ttag_scores = {} # tag : {TP: count , FP: count, TN: count, FN: count}\n",
        "\tfor tag in all_tags:\n",
        "\t  index = all_tags_index[tag] # tag to an index\n",
        "\t  scores={}\n",
        "\t  tp = confusion_matrix[index][index]\n",
        "\t  scores[\"TP\"] = tp\n",
        "\t  fp = predicted_totals[index] - tp\n",
        "\t  scores[\"FP\"] = fp\n",
        "\t  fn = actual_totals[index] - tp\n",
        "\t  scores[\"FN\"] = fn\n",
        "\t  tag_scores[tag] = scores\n",
        "\n",
        "\t#===========================================================\n",
        "\t#\tCALCULATING TAG-WISE PRECISION, RECALL, F1-SCORE\n",
        "\t#===========================================================\n",
        "\tmatrices={}\n",
        "\tfor tag in all_tags:\n",
        "\t  c = tag_scores[tag]\n",
        "\t  precision = c[\"TP\"]/(c[\"TP\"]+c[\"FP\"])\n",
        "\t  precision = 0 if math.isnan(precision) else precision\n",
        "\t  recall = c[\"TP\"]/(c[\"TP\"]+c[\"FN\"])\n",
        "\t  recall = 0 if math.isnan(recall) else recall\n",
        "\t  temp = [precision,recall]\n",
        "\t  f1_score = hm(temp)\n",
        "\t  matrices[tag] = [precision,recall, f1_score]\n",
        "\n",
        "\t  if tag in matrices_across_folds:\n",
        "\t    matrices_across_folds[tag][phase] = [precision,recall, f1_score]\n",
        "\t  else:\n",
        "\t    matrices_across_folds[tag] = {phase:[precision,recall, f1_score]}\n",
        "\n",
        "\tprint(\"Tag-wise precision, recall and f1-score\")\n",
        "\tprint(matrices)\n",
        "\n",
        "\n",
        "\t#===========================================================\n",
        "\t#\tCALCULATING PRECISION, RECALL, F1-SCORE\n",
        "\t#===========================================================\n",
        "\toverall_matrices=[]\n",
        "\tall_precisions = []\n",
        "\tall_recalls=[]\n",
        "\tall_f1_scores=[]\n",
        "\tfor k in matrices.keys():\n",
        "\t  a = matrices[k]\n",
        "\t  all_precisions.append(a[0])\n",
        "\t  all_recalls.append(a[1])\n",
        "\t  all_f1_scores.append(a[2])\n",
        "\n",
        "\toverall_matrices.append(get_average(all_precisions))\n",
        "\toverall_matrices.append(get_average(all_recalls))\n",
        "\toverall_matrices.append(get_average(all_f1_scores))\n",
        "\n",
        "\tprint(\"Overall precision, recall and f1-score\")\n",
        "\tprint(overall_matrices)\n",
        "\toverall_matrices_across_folds[phase] = overall_matrices\n",
        "\n",
        "#===========================================================\n",
        "#\t\t\t\t\t3 FOLDS END HERE\n",
        "#===========================================================\n",
        "\n",
        "tagwise_average_scores = {}\n",
        "for k in matrices_across_folds.keys(): # k = tag\n",
        "\tphase_wise = list(matrices_across_folds[k].values()) #[[p1,r1,f1],[p2,r2,f2],[p3,r3,f3]]\n",
        "\tavg_pre = 0\n",
        "\tavg_rec = 0\n",
        "\tavg_f1 = 0\n",
        "\tfor i in phase_wise:\n",
        "\t\tavg_pre+=i[0]\n",
        "\t\tavg_rec+=i[1]\n",
        "\t\tavg_f1+=i[2]\n",
        "\tavg_pre/=3\n",
        "\tavg_rec/=3\n",
        "\tavg_f1/=3\n",
        "\ttagwise_average_scores[k] = [avg_pre,avg_rec,avg_f1]\n",
        "\n",
        "average_scores = []\n",
        "phase_wise = list(overall_matrices_across_folds.values()) #[[p1,r1,f1],[p2,r2,f2],[p3,r3,f3]]\n",
        "avg_pre = 0\n",
        "avg_rec = 0\n",
        "avg_f1 = 0\n",
        "for i in phase_wise:\n",
        "\tavg_pre+=i[0]\n",
        "\tavg_rec+=i[1]\n",
        "\tavg_f1+=i[2]\n",
        "avg_pre/=3\n",
        "avg_rec/=3\n",
        "avg_f1/=3\n",
        "average_scores = [avg_pre,avg_rec,avg_f1]\n",
        "\n",
        "tagwise_average_frequency={}\n",
        "for k in frequnecy_distribution_across_folds.keys():\n",
        "\tv = frequnecy_distribution_across_folds[k]\n",
        "\tfor k2 in v.keys(): #tag\n",
        "\t\ttagwise_average_frequency[k2] = tagwise_average_frequency.get(k2,0)+v[k2]\n",
        "\n",
        "for k in tagwise_average_frequency:\n",
        "\ttagwise_average_frequency[k]/=3\n",
        "\n",
        "tagwise_average_percentage={}\n",
        "for k in percentage_distribution_across_folds.keys():\n",
        "\tv = percentage_distribution_across_folds[k]\n",
        "\tfor k2 in v.keys(): #tag\n",
        "\t\ttagwise_average_percentage[k2] = tagwise_average_percentage.get(k2,0)+v[k2]\n",
        "\n",
        "for k in tagwise_average_percentage:\n",
        "\ttagwise_average_percentage[k]/=3\n",
        "\n",
        "A=0\n",
        "B=0\n",
        "for k in ambiguous_matrix_across_folds.keys():\n",
        "\ta,b = ambiguous_matrix_across_folds[k]\n",
        "\tA+=a\n",
        "\tB+=b\n",
        "A/=3\n",
        "B/=3\n",
        "\n",
        "print(\"Average scores across the 3 folds\")\n",
        "print(\"Tag-wise\")\n",
        "print(tagwise_average_scores)\n",
        "print(\"Overall\")\n",
        "print(average_scores)\n",
        "print(\"Average frequency distribution of tags across folds\")\n",
        "print(tagwise_average_frequency)\n",
        "print(\"Average percentage of words that are ambiguous: \",A)\n",
        "print(\"Average percentags occurence of such ambiguous words: \",B)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4hxVzjdUFdv"
      },
      "source": [
        "# **Markov assumption length 2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLCSRzplTznL"
      },
      "source": [
        "# PRE-PROCESSING \n",
        "new=[]\n",
        "start=[\"^_^\"]\n",
        "end=[\"._.\"]\n",
        "for i in processed_lines:\n",
        "  new.append(start+i+end)\n",
        "\n",
        "# PARTIONING\n",
        "length = len(new)\n",
        "mod = length%3\n",
        "part = int((length-mod)/3)\n",
        "part1 = new[:part+mod]\n",
        "part2 = new[part+mod:(2*part)+mod]\n",
        "part3 = new[(2*part)+mod:]\n",
        "parts=[part1,part2,part3]\n",
        "\n",
        "matrices_across_folds = {}\n",
        "overall_matrices_across_folds = {}\n",
        "frequnecy_distribution_across_folds = {}\n",
        "percentage_distribution_across_folds = {}\n",
        "ambiguous_matrix_across_folds = {}\n",
        "\n",
        "for phase in range(3): # 0,1,2\n",
        "\ttrain = parts[(phase)%3] + parts[(phase+1)%3]\n",
        "\ttest = parts[(phase+2)%3]\n",
        "\n",
        "\t# PRE-PROCESSING OF TOKENS\n",
        "\ttag_set = set([])  # all the tags from train data\n",
        "\tword_set = set([]) # all words from train data\n",
        "\tfor i in train:\n",
        "\t\ttokens = i\n",
        "\t\tfor j in tokens:\n",
        "\t\t\tword_tag = j.split(\"_\")\n",
        "\t\t\ttag_set.add(word_tag[1])\n",
        "\t\t\tword_set.add(word_tag[0]) \n",
        "\n",
        "\ttags = list(tag_set)\n",
        "\ttemplate_tags = []\n",
        "\ttags_index = {}\n",
        "\tl = len(tags)\n",
        "\tfor i in range(l):\n",
        "\t\ttemplate_tags.append(0)\n",
        "\t\ttags_index[tags[i]] = i\n",
        "\n",
        "\twords = list(word_set)\n",
        "\ttemplate_words = []\n",
        "\twords_index = {}\n",
        "\tl = len(words)\n",
        "\tfor i in range(l):\n",
        "\t\ttemplate_words.append(0)\n",
        "\t\twords_index[words[i]] = i\n",
        "\n",
        "\t# FINDING EMISSION AND TRANSITION COUNTS\n",
        "\temission_counts = {} # tag : [countS corresponding to all words]\n",
        "\ttotal_counts_for_all_tags = {} \n",
        "\n",
        "\tfor i in tags:\n",
        "\t\temission_counts[i] = template_words[:]\n",
        "\n",
        "\ttotal_transitions = 0 \n",
        "\ttransition_counts = {} # {state1:  {state2 : count }}\n",
        "\n",
        "\tambiguity={}\n",
        "\toccurences={}\n",
        "\n",
        "\tfor i in tags:\n",
        "\t\ttemp = {}\n",
        "\t\tfor j in tags:\n",
        "\t\t\ttemp[j]=0\n",
        "\t\ttransition_counts[i]=temp\n",
        "\n",
        "\tfor i in train:\n",
        "\t  tokens = i\n",
        "\t  for j in range(len(tokens)-1):\n",
        "\t    token = tokens[j]\n",
        "\t    word_tag = token.split(\"_\")\n",
        "\t    s1 = word_tag[1]\n",
        "\t    token = tokens[j+1]\n",
        "\t    word_tag = token.split(\"_\")\n",
        "\t    s2 = word_tag[1]\n",
        "\t    total_trans += 1\n",
        "\t    transition_counts[s1][s2] += 1\n",
        "\n",
        "\tfor i in train:\n",
        "\t\ttokens = i\n",
        "\t\tfor j in tokens:\n",
        "\t\t\tword_tag = j.split(\"_\")\n",
        "\t\t\tword = word_tag[0]\n",
        "\t\t\ttag = word_tag[1]\n",
        "\t\t\tindex = words_index[word]\n",
        "\t\t\temission_counts[tag][index] += 1\n",
        "\t\t\tindex = tags_index[tag]\n",
        "\t\t\ttotal_counts_for_all_tags[tag] = total_counts_for_all_tags.get(tag,0)+1\n",
        "\t\t\tif word in occurences:\n",
        "\t\t\t  occurences[word]+=1\n",
        "\t\t\t  ambiguity[word].add(tag)\n",
        "\t\t\telse:\n",
        "\t\t\t  occurences[word]=1\n",
        "\t\t\t  ambiguity[word]=set([tag])\n",
        "\n",
        "\n",
        "\ttotal_trans = 0 \n",
        "\ttrans_counts = {} #{s1: {s2 : {s3:count }}}\n",
        "\n",
        "\t# INITIALIZING\n",
        "\tfor t1 in tag_set:\n",
        "\t  a={}\n",
        "\t  for t2 in tag_set:\n",
        "\t    b={}\n",
        "\t    for t3 in tag_set:\n",
        "\t      b[t3] = 0\n",
        "\t    a[t2] = b\n",
        "\t  trans_counts[t1] = a\n",
        "\n",
        "\t# FILLING\n",
        "\tfor i in train:\n",
        "\t  tokens = i\n",
        "\t  for j in range(len(tokens)-2):\n",
        "\t    token = tokens[j]\n",
        "\t    word_tag = token.split(\"_\")\n",
        "\t    s1 = word_tag[1]\n",
        "\t    token = tokens[j+1]\n",
        "\t    word_tag = token.split(\"_\")\n",
        "\t    s2 = word_tag[1]\n",
        "\t    token = tokens[j+2]\n",
        "\t    word_tag = token.split(\"_\")\n",
        "\t    s3 = word_tag[1]    \n",
        "\t    total_trans += 1\n",
        "\t    trans_counts[s1][s2][s3] +=1\n",
        "\n",
        "\n",
        "\t#===========================================================\n",
        "\t#\t\t\t\t\tTESTING DATA IS SEEN HERE\n",
        "\t#===========================================================\n",
        "\tnew_words = set([]) # have zero occurence counts\n",
        "\tnew_tags = set([]) # have zero occurence counts\n",
        "\n",
        "\t# SEPARATING WORDS AND TAGS OF THE TEST DATA\n",
        "\ttest_data=[]\n",
        "\tactual_tags = []\n",
        "\tfor i in test:\n",
        "\t\ttokens = i\n",
        "\t\twords=[]\n",
        "\t\ttags=[]\n",
        "\t\tfor j in tokens:\n",
        "\t\t\tword_tag = j.split(\"_\")\n",
        "\t\t\tword = word_tag[0]\n",
        "\t\t\ttag = word_tag[1]\n",
        "\t\t\twords.append(word)\n",
        "\t\t\ttags.append(tag)\n",
        "\t\t\tif word not in word_set:\n",
        "\t\t\t\tnew_words.add(word)\n",
        "\t\t\tif tag not in tag_set:\n",
        "\t\t\t\tnew_tags.add(tag)\n",
        "\t\ttest_data.append(words)\n",
        "\t\tactual_tags.append(tags)\n",
        "\n",
        "\t# FINDING TOTAL SIZES OF VOCABULARIES\n",
        "\tall_words = set(list(word_set)+list(new_words))\n",
        "\tvocab_of_words = len(all_words)\n",
        "\tall_tags = set(list(tag_set)+list(new_tags))\n",
        "\tvocab_of_tags = len(all_tags)\n",
        "\t# transition_size = (len(all_tags))**3\n",
        "\n",
        "\t#===========================================================\n",
        "\t#\t\tCALCULATING PROBABILITIES WITH SMOOTHING\n",
        "\t#===========================================================\n",
        "\ttotal_tag_counts = sum(list(total_counts_for_all_tags.values()))\n",
        "\n",
        "\t# PROBABILITY OF OCCURENCE OF A TAG\n",
        "\ttag_probabs = {} \n",
        "\tfor k in emission_counts.keys(): # k = tag\n",
        "\t  v = sum(emission_counts[k])\n",
        "\t  tag_probabs[k] = (v+1)/(total_tag_counts + vocab_of_tags)\n",
        "\tfor i in new_tags:\n",
        "\t  tag_probabs[i] = 1/(total_tag_counts + vocab_of_tags)\n",
        "\n",
        "\t# PROBABILITY OF A WORD GIVEN A TAG\n",
        "\tword_given_tag_probabs={}  #tag: {word : count}\n",
        "\tfor i in tag_set:\n",
        "\t  word_probabs={}\n",
        "\t  for j in word_set:\n",
        "\t    index = words_index[j]\n",
        "\t    word_probabs[j] = (emission_counts[i][index] +1)/(tag_probabs[i]*total_tag_counts + vocab_of_words)\n",
        "\t  for j in new_words:\n",
        "\t    word_probabs[j] = 1/(tag_probabs[i]*total_tag_counts + vocab_of_words)\n",
        "\t  word_given_tag_probabs[i] = word_probabs\n",
        "\n",
        "\tfor i in new_tags:\n",
        "\t  word_probabs={}\n",
        "\t  for j in all_words:\n",
        "\t    word_probabs[j] = 1/(tag_probabs[i]*total_tag_counts + vocab_of_words)\n",
        "\t  word_given_tag_probabs[i] = word_probabs\n",
        "\n",
        "\n",
        "\t# PROBABILITY OF A STATE(SAY 2) GIVEN PREVIOUS STATE(SAY 1)\n",
        "\ts2_given_s1_probabs={} #tag2: {tag1 : count}\n",
        "\tfor t2 in tag_set:\n",
        "\t  t1_probabs={}\n",
        "\t  for t1 in tag_set:\n",
        "\t    t1_probabs[t1] = (transition_counts[t1][t2] + 1)/(tag_probabs[t1]*total_tag_counts + total_trans)\n",
        "\t  for t1 in new_tags:\n",
        "\t    t1_probabs[t1] = 1/(tag_probabs[t1]*total_tag_counts + total_trans)\n",
        "\t  s2_given_s1_probabs[t2] = t1_probabs\n",
        "\n",
        "\tfor t2 in new_tags:\n",
        "\t  t1_probabs={}\n",
        "\t  for t1 in all_tags:\n",
        "\t    t1_probabs[t1] = 1/(tag_probabs[t1]*total_tag_counts + total_trans)\n",
        "\t  s2_given_s1_probabs[t2] = t1_probabs\n",
        "\n",
        "\t  # PROBABILITY OF s3 given (s1,s2) IN LOG SPACE\n",
        "\ts3_given_s2_and_s1_probabs={} #smoothed tag3: {(tag1,tag2) : count}\n",
        "\tfor t3 in all_tags:\n",
        "\t  t2_and_t1_probabs={}\n",
        "\t  for t2 in all_tags:\n",
        "\t    # t1_probabs={}\n",
        "\t    for t1 in all_tags:\n",
        "\t      try:\n",
        "\t        c = trans_counts[t1][t2][t3]\n",
        "\t      except KeyError as ke:\n",
        "\t        c=0\n",
        "\t      t2_and_t1_probabs[(t1,t2)] = log10((c + 1)/(s2_given_s1_probabs[t2][t1]*(tag_probabs[t1]*total_tag_counts) + total_trans))\n",
        "\t  s3_given_s2_and_s1_probabs[t3] = t2_and_t1_probabs\n",
        "\n",
        "\t#===========================================================\n",
        "\t#\t\t\t\tCALCULATING STATS\n",
        "\t#===========================================================\n",
        "\n",
        "\tfor tag in new_tags:\n",
        "\t  total_counts_for_all_tags[tag] = 0 #frequencies\n",
        "\n",
        "\tprint(\"Statistics of tags: frequency distribution:\")\n",
        "\tprint(total_counts_for_all_tags)\n",
        "\tfrequnecy_distribution_across_folds[phase] = total_counts_for_all_tags\n",
        "\n",
        "\tpercentages={}\n",
        "\tfor i in all_tags:\n",
        "\t  percentages[i] = (total_counts_for_all_tags[i] * 100)/total_tag_counts\n",
        "\tprint(\"Statistics of tags: percentage distribution:\")\n",
        "\tprint(percentages)\n",
        "\tpercentage_distribution_across_folds[phase] = percentages\n",
        "\n",
        "\n",
        "\tambiguous={}\n",
        "\tfor k in ambiguity.keys():\n",
        "\t  if len(ambiguity[k])>1:\n",
        "\t    ambiguous[k]=len(ambiguity[k])\n",
        "\n",
        "\ttotal_occurences = sum(list(occurences.values()))\n",
        "\tambiguous_occurences={}\n",
        "\tfor k in ambiguous.keys():\n",
        "\t  ambiguous_occurences[(k,ambiguous[k])] = (100*occurences[k])/total_occurences\n",
        "\t# print(\"Words | Tags | Percentage of occurences\")\n",
        "\t# print(ambiguous_occurences)\n",
        "\ta=(len(ambiguous)*100)/len(all_words)\n",
        "\tb=sum(list(ambiguous_occurences.values()))\n",
        "\tprint(\"Percentage of words that are ambiguous\")\n",
        "\tprint(a)\n",
        "\tprint(\"Total occurences percentage of ambiguous words\")\n",
        "\tprint(b)\n",
        "\tambiguous_matrix_across_folds[phase] = [a,b]\n",
        "\n",
        "\t#===========================================================\n",
        "\t#\t\t\t\tTESTING\n",
        "\t#===========================================================\n",
        "\tresults=[]\n",
        "\n",
        "\tfor i in test_data:\n",
        "\t  result=[]\n",
        "\t  tokens = i\n",
        "\t  entry_tag = tokens[1]\n",
        "\t  probabilities1 = {}\n",
        "\t  for t in all_tags:\n",
        "\t    probabilities1[\"^ ^\"] = log10(1)\n",
        "\n",
        "\t  #P(s|o)\n",
        "\t  for j in range(len(tokens)):\n",
        "\t    old_word = tokens[j]\n",
        "\t    #p(o1|s1)\n",
        "\t    probab_word_given_tags={}\n",
        "\t    for t in all_tags:\n",
        "\t      probab_word_given_tags[t] = log10(word_given_tag_probabs[t][old_word])\n",
        "\n",
        "\t    #p(s3|s2 and s1) \n",
        "\t    #s3_given_s2_and_s1_probabs\n",
        "\n",
        "\t    #GOAL: p(o1|s1)*p(older(till s1)) = P\n",
        "\t    P = {} # s1 : p(o1|s1)*p(older(till s1))\n",
        "\t    for k in probabilities1.keys(): \n",
        "\t      tags = k.split(\" \")\n",
        "\t      last_tag = tags[-2] # gives s1\n",
        "\t      P[last_tag] = probabilities1[k] + probab_word_given_tags[last_tag] # both already in log space\n",
        "\t      \n",
        "\t    next_probabilities={}\n",
        "\t    for t3 in all_tags:\n",
        "\t      probabs = {}\n",
        "\t      for k in probabilities1.keys():\n",
        "\t        v = probabilities1[k]\n",
        "\t        l = k.split(\" \")\n",
        "\t        t1 = l[-2]\n",
        "\t        t2 = l[-1]\n",
        "\t        probabs[k] = P[t1] + s3_given_s2_and_s1_probabs[t3][(t1,t2)] # already in log space\n",
        "\n",
        "\t      key_max = max(probabs, key=probabs.get) # key_max = t1 \n",
        "\t      # key_min = min(probabs, key=probabs.get) # key_max = t1 \n",
        "\t      # print(probabs[key_min])\n",
        "\t      value_max = probabs[key_max]\n",
        "\t      l = key_max.split(\" \")\n",
        "\t      l.append(t3)\n",
        "\t      v = value_max \n",
        "\t      k = \" \".join(i for i in l)\n",
        "\t      next_probabilities[k]=v\n",
        "\t    probabilities1 = next_probabilities\n",
        "\t    \n",
        "\t  key_max = max(probabilities1, key=probabilities1.get) # best tag set\n",
        "\t  result = key_max\n",
        "\t  results.append(result.split(\" \")[:-2])\n",
        "\tprint(results)\n",
        "\t#===========================================================\n",
        "\t#\t\t\t\tCALCULATING MATRICES\n",
        "\t#===========================================================\n",
        "\n",
        "\tconfusion_matrix = np.zeros((dimension,dimension))\n",
        "\n",
        "\tfor i in range(len(actual_tags)):\n",
        "\t  for j in range(len(actual_tags[i])):\n",
        "\t    at = actual_tags[i][j]\n",
        "\t    pt = results[i][j]\n",
        "\t    ati = all_tags_index[at]\n",
        "\t    pti = all_tags_index[pt]\n",
        "\t    confusion_matrix[ati][pti]+=1\n",
        "\n",
        "\ttotal = np.sum(confusion_matrix)\n",
        "\tpredicted_totals = np.sum(confusion_matrix, axis = 0)\n",
        "\tactual_totals = np.sum(confusion_matrix, axis = 1)\n",
        "\n",
        "\t# print(total)\n",
        "\n",
        "\n",
        "\ttag_scores = {} # tag : {TP: count , FP: count, TN: count}\n",
        "\tfor tag in all_tags:\n",
        "\t  index = all_tags_index[tag] # tag to an index\n",
        "\t  scores={}\n",
        "\t  tp = confusion_matrix[index][index]\n",
        "\t  scores[\"TP\"] = tp\n",
        "\t  fp = predicted_totals[index] - tp\n",
        "\t  scores[\"FP\"] = fp\n",
        "\t  fn = actual_totals[index] - tp\n",
        "\t  scores[\"FN\"] = fn\n",
        "\t  # tn = total2 - actual_totals[index] - predicted_totals[index] + tp\n",
        "\t  # scores[\"TN\"] = tn\n",
        "\t  tag_scores[tag] = scores\n",
        "\n",
        "\tmatrices={}\n",
        "\tfor tag in all_tags:\n",
        "\t  c = tag_scores[tag]\n",
        "\t  precision = c[\"TP\"]/(c[\"TP\"]+c[\"FP\"])\n",
        "\t  precision = 0 if math.isnan(precision) else precision\n",
        "\t  recall = c[\"TP\"]/(c[\"TP\"]+c[\"FN\"])\n",
        "\t  recall = 0 if math.isnan(recall) else recall\n",
        "\t  temp = [precision,recall]\n",
        "\t  f1_score = hm(temp)\n",
        "\t  matrices[tag] = [precision,recall, f1_score]\n",
        "\t  matrices_across_folds[tag] = {phase:[precision,recall,f1_score]}\n",
        "\n",
        "\n",
        "\tprint(\"Tag-wise precision, recall and f1-score\")\n",
        "\tprint(matrices)\n",
        "\n",
        "\toverall_matrices=[]\n",
        "\tall_precisions = []\n",
        "\tall_recalls=[]\n",
        "\tall_f1_scores=[]\n",
        "\tfor k in matrices.keys():\n",
        "\t  a = matrices[k]\n",
        "\t  all_precisions.append(a[0])\n",
        "\t  all_recalls.append(a[1])\n",
        "\t  all_f1_scores.append(a[2])\n",
        "\n",
        "\toverall_matrices.append(get_average(all_precisions))\n",
        "\toverall_matrices.append(get_average(all_recalls))\n",
        "\toverall_matrices.append(get_average(all_f1_scores))\n",
        "\n",
        "\toverall_matrices_across_folds[phase] = overall_matrices\n",
        "\n",
        "\tprint(\"Precision, recall and f1-score\")\n",
        "\tprint(overall_matrices)\n",
        "\n",
        "\n",
        "#===========================================================\n",
        "#\t\t\t\t\t3 FOLDS END HERE\n",
        "#===========================================================\n",
        "\n",
        "tagwise_average_scores = {}\n",
        "for k in matrices_across_folds.keys(): # k = tag\n",
        "\tphase_wise = list(matrices_across_folds[k].values()) #[[p1,r1,f1],[p2,r2,f2],[p3,r3,f3]]\n",
        "\tavg_pre = 0\n",
        "\tavg_rec = 0\n",
        "\tavg_f1 = 0\n",
        "\tfor i in phase_wise:\n",
        "\t\tavg_pre+=i[0]\n",
        "\t\tavg_rec+=i[1]\n",
        "\t\tavg_f1+=i[2]\n",
        "\tavg_pre/=3\n",
        "\tavg_rec/=3\n",
        "\tavg_f1/=3\n",
        "\ttagwise_average_scores[k] = [avg_pre,avg_rec,avg_f1]\n",
        "\n",
        "average_scores = []\n",
        "phase_wise = list(overall_matrices_across_folds.values()) #[[p1,r1,f1],[p2,r2,f2],[p3,r3,f3]]\n",
        "avg_pre = 0\n",
        "avg_rec = 0\n",
        "avg_f1 = 0\n",
        "for i in phase_wise:\n",
        "\tavg_pre+=i[0]\n",
        "\tavg_rec+=i[1]\n",
        "\tavg_f1+=i[2]\n",
        "avg_pre/=3\n",
        "avg_rec/=3\n",
        "avg_f1/=3\n",
        "average_scores = [avg_pre,avg_rec,avg_f1]\n",
        "\n",
        "tagwise_average_frequency={}\n",
        "for k in frequnecy_distribution_across_folds.keys():\n",
        "\tv = frequnecy_distribution_across_folds[k]\n",
        "\tfor k2 in v.keys(): #tag\n",
        "\t\ttagwise_average_frequency[k2] = tagwise_average_frequency.get(k2,0)+v[k2]\n",
        "\n",
        "for k in tagwise_average_frequency:\n",
        "\ttagwise_average_frequency[k]/=3\n",
        "\n",
        "tagwise_average_percentage={}\n",
        "for k in percentage_distribution_across_folds.keys():\n",
        "\tv = percentage_distribution_across_folds[k]\n",
        "\tfor k2 in v.keys(): #tag\n",
        "\t\ttagwise_average_percentage[k2] = tagwise_average_percentage.get(k2,0)+v[k2]\n",
        "\n",
        "for k in tagwise_average_percentage:\n",
        "\ttagwise_average_percentage[k]/=3\n",
        "\n",
        "A=0\n",
        "B=0\n",
        "for k in ambiguous_matrix_across_folds.keys():\n",
        "\ta,b = ambiguous_matrix_across_folds[k]\n",
        "\tA+=a\n",
        "\tB+=b\n",
        "A/=3\n",
        "B/=3\n",
        "\n",
        "print(\"Average scores across the 3 folds\")\n",
        "print(\"Tag-wise\")\n",
        "print(tagwise_average_scores)\n",
        "print(\"Overall\")\n",
        "print(average_scores)\n",
        "print(\"Average frequency distribution of tags across folds\")\n",
        "print(tagwise_average_frequency)\n",
        "print(\"Average percentage of words that are ambiguous: \",A)\n",
        "print(\"Average percentags occurence of such ambiguous words: \",B)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}